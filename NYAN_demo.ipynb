{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1eb6a7",
   "metadata": {},
   "source": [
    "![Logo](art/NYAN.png \"Title\")\n",
    "## NotYetAnotherNightshade practical demonstration\n",
    "As part of the manuscript \"*Application of variational graph encoders as an effective generalist algorithm in holistic computer-aided drug design*\".\n",
    "\n",
    "We thank the anonymous reviewer for suggesting this!\n",
    "\n",
    "**This notebook will go through the following:**\n",
    "1. Downloading data from the TDCommons dataset, and converting it into latent space\n",
    "2. Training a simple model on the latent space\n",
    "3. Saving and loading models\n",
    "4. Potentiating molecules based off existing uploaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTDC, Matplotlib and Seaborn are not part of the original Conda environment\n",
    "# As I wanted to keep it minimal\n",
    "# You'll have to install it here.\n",
    "\n",
    "!pip3 install PyTDC\n",
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set environment variables\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import loading\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(\"__file__\")) + \"/\"\n",
    "\n",
    "from data import BondType\n",
    "\n",
    "import logging\n",
    "\n",
    "from tdc.single_pred import ADME\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from spektral.data import BatchLoader, Dataset, Graph\n",
    "from spektral import transforms\n",
    "\n",
    "from model import VAE, NyanEncoder, NyanDecoder, EpochCounter\n",
    "\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "logging.getLogger(\"pysmiles\").setLevel(logging.CRITICAL)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "save = dir_path + \"saves/ZINC-extmodel5hk-3M\"\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81887db2",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "Downloading data from the TDCommons dataset, and converting it into latent space.\n",
    "\n",
    "(We usually call this latentisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please make sure that you have your Conda environment previously instantiated!\n",
    "# I will use the dataset BBB_Martins as an example\n",
    "dataset_name = \"BBB_Martins\"\n",
    "data = ADME(name=dataset_name)\n",
    "\n",
    "# Please take note! I are using random split here instead of scaffold split.\n",
    "# You can refer to Supplementary File 1 for scaffold split results.\n",
    "# The reason why random split is done here is for fivefold cross-validation.\n",
    "split = data.get_split(method = \"random\")\n",
    "\n",
    "# Combine all the data together in this case since I'm doing CV\n",
    "data = split[\"train\"].values.tolist() + split[\"valid\"].values.tolist() + split[\"test\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302357a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    encoder = NyanEncoder(latent_dim=64, batched=True)\n",
    "    decoder = NyanDecoder(fingerprint_bits=679, regression=1613)\n",
    "\n",
    "    model = VAE(encoder, decoder)\n",
    "    model.load_weights(save).expect_partial()\n",
    "\n",
    "print(\"Generating latents using the save {}\".format(save))\n",
    "\n",
    "all_smiles = data\n",
    "\n",
    "# Initialise dataset\n",
    "graph_data = list()\n",
    "passed = list()\n",
    "\n",
    "print(\"Loading {} molecules\".format(len(all_smiles)))\n",
    "\n",
    "for smile in tqdm(all_smiles):\n",
    "\n",
    "    if smile[1] == \"\":\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "\n",
    "        graph = loading.get_data(smile[1], only_biggest=True, unknown_atom_is_dummy=True)\n",
    "\n",
    "        x, a, e = loading.convert(*graph, bonds=[BondType.SINGLE, BondType.DOUBLE, BondType.TRIPLE, BondType.AROMATIC, BondType.NOT_CONNECTED])\n",
    "\n",
    "        graph = Graph(x=np.array(x), a=np.array(a), e=np.array(e), y=np.array(0))\n",
    "\n",
    "        graph_data.append([graph, None])\n",
    "\n",
    "        passed.append(smile)\n",
    "\n",
    "    except Exception as error:\n",
    "\n",
    "        print(\"Errored loading SMILES\", smile)\n",
    "\n",
    "class EvalDataset (Dataset):\n",
    "\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read (self):\n",
    "        return [x[0] for x in graph_data]\n",
    "\n",
    "dataset = EvalDataset()\n",
    "loader = BatchLoader(dataset, batch_size=batch_size, epochs=1, mask=True, shuffle=False, node_level=False)\n",
    "\n",
    "predictions = encoder.predict(loader.load())\n",
    "predictions = [[float(y) for y in x] for x in predictions]\n",
    "\n",
    "all_data = list()\n",
    "\n",
    "for i in range(len(passed)):\n",
    "\n",
    "    current_smiles = passed[i]\n",
    "\n",
    "    appendable = [current_smiles] + predictions[i]\n",
    "    all_data.append(appendable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b590d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have latentised the variables, let's look at the first molecules\n",
    "\n",
    "print(all_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d9fd9",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "Training a model and performing cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually I shuffle the data again before cross-validation, just in case\n",
    "# Don't want to introduce biases unknowingly.\n",
    "\n",
    "random.Random(0).shuffle(all_data)\n",
    "\n",
    "# Fivefold cross-validation, as per manuscript\n",
    "cross_validation_runs = 5\n",
    "\n",
    "results = {\"AUROC\": list(), \"AUPRC\": list(), \"accuracy\": list()}\n",
    "\n",
    "for i in range(cross_validation_runs):\n",
    "\n",
    "    bottom = math.ceil(i/cross_validation_runs * len(all_data))\n",
    "    top = math.ceil((i + 1)/cross_validation_runs * len(all_data))\n",
    "    \n",
    "    training_set = all_data[:bottom] + all_data[top:]\n",
    "    testing_set = all_data[bottom:top]\n",
    "\n",
    "\n",
    "    all_labels = sorted(list(set([x[0][2] for x in all_data])))\n",
    "    \n",
    "    training_latent_space = np.array([[float(y) for y in x[1:]] for x in training_set])\n",
    "    training_labels = [all_labels.index(x[0][2]) for x in training_set]\n",
    "\n",
    "    # Cap the latent spaces to [-1e8, 1e8]\n",
    "    \n",
    "    training_latent_space[training_latent_space == -np.inf] = -1e8\n",
    "    training_latent_space[training_latent_space == np.inf] = 1e8\n",
    "    \n",
    "    # This is the part where you decide what kind of model you want to use\n",
    "    # In this case, I'll follow what we originally did in the manuscript for the BBB_Martins dataset\n",
    "    # Which is to use ExtraTreesClassifiers\n",
    "    # You can use other pipelines of interest, even autoML methods\n",
    "\n",
    "    clf = make_pipeline(RobustScaler(quantile_range=(10, 90), unit_variance=True), ExtraTreesClassifier(n_estimators=512, max_features=\"log2\", n_jobs=-1, random_state=0))\n",
    "    clf.fit(training_latent_space, training_labels)\n",
    "\n",
    "    testing_latent_space = np.array([[float(y) for y in x[1:]] for x in testing_set])\n",
    "    testing_labels = [all_labels.index(x[0][2]) for x in testing_set]\n",
    "    \n",
    "    # Test the model\n",
    "    y_score = clf.predict_proba(testing_latent_space)\n",
    "    y_test = testing_labels\n",
    "\n",
    "    # Convert y_test to one-hot\n",
    "    def create_one_hot(size, index):\n",
    "\n",
    "        vector = [0] * size\n",
    "        vector[index] = 1\n",
    "\n",
    "        return vector\n",
    "\n",
    "    y_test = np.array([create_one_hot(len(all_labels), x) for x in y_test])\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(len(all_labels)):\n",
    "\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    results[\"AUROC\"].append(roc_auc[0])\n",
    "\n",
    "    # Strictly not equivalent, but more pessimistic\n",
    "    auprc = metrics.average_precision_score(y_test[:, 0], y_score[:, 0])\n",
    "    results[\"AUPRC\"].append(auprc)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test[:, 0], np.round(y_score[:, 0]))\n",
    "    results[\"accuracy\"].append(accuracy)\n",
    "\n",
    "# Fivefold CV results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1caa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the means and standard deviations\n",
    "\n",
    "auroc_mean = str(np.mean(results[\"AUROC\"]))\n",
    "auroc_std = str(np.std(results[\"AUROC\"]))\n",
    "\n",
    "auprc_mean = str(np.mean(results[\"AUPRC\"]))\n",
    "auprc_std = str(np.std(results[\"AUPRC\"]))\n",
    "\n",
    "accuracy_mean = str(np.mean(results[\"accuracy\"]))\n",
    "accuracy_std = str(np.std(results[\"accuracy\"]))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_mean} ± {accuracy_std}\")\n",
    "print(f\"AUROC: {auroc_mean} ± {auroc_std}\")\n",
    "print(f\"AUPRC: {auprc_mean} ± {auprc_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65399369",
   "metadata": {},
   "source": [
    "#### Part 3\n",
    "Saving a model to Joblib so that it can be retrieved again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "# I'm going to take the last model from the CV and save it.\n",
    "# Of course, in practicable situations, you may want to fine-tune the exact\n",
    "# train and test set\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(clf, \"models/\" + dataset_name + \".joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd003b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model is as simple as instantiating the Joblib\n",
    "\n",
    "clf = joblib.load(\"models/\" + dataset_name + \".joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ac670",
   "metadata": {},
   "source": [
    "#### Part 4\n",
    "Potentiating molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use model which we have previously trained\n",
    "\n",
    "# Here we try to potentiate a molecule that can pass the blood brain barrier\n",
    "# I have blanked out the solubiity part since the model file itself is 2 GB,\n",
    "# but it is there for your reference in case you need to do regression model potentiations.\n",
    "\n",
    "# I deliberately soften out the labels in the preferences also, this is especially important when\n",
    "# you have many, many parameters you want to optimise at once.\n",
    "\n",
    "model_directory = \"models/\"\n",
    "\n",
    "preferences = [\n",
    "\n",
    "  #{\"model\": \"Solubility_AqSolDB\", \"type\": \"regression\", \"weight\": 1, \"cutoff\": 4, \"offset\": -8, \"further-optimisation\": false},\n",
    "  {\"model\": \"BBB_Martins\", \"type\": \"classification\", \"weight\": [1, 0], \"cutoff\": [-0.6, 0.6], \"offset\": [0, 0], \"further-optimisation\": [True, True]}\n",
    "    \n",
    "]\n",
    "\n",
    "# In the paper we did 10,000 iterations\n",
    "# Here I'm just going to do 1,000 so that this doesn't take forever to run\n",
    "# for smaller computers\n",
    "\n",
    "iterations = 1000\n",
    "step_std = 0.01\n",
    "\n",
    "# Metropolis-Hastings configuration\n",
    "\n",
    "beta = 1\n",
    "annealing_alpha = 0.99\n",
    "step_alpha = 1\n",
    "\n",
    "latent = [0] * 64\n",
    "latent = [x + random.gauss(0, 0.5) for x in latent]\n",
    "\n",
    "trajectory = list()\n",
    "\n",
    "previous_score = float(\"-inf\")\n",
    "\n",
    "def cost_function (x, cutoff, offset=0, allow_further_opt=True):\n",
    "\n",
    "    if cutoff == 0:\n",
    "        raise Exception(\"Cutoff cannot be zero\")\n",
    "\n",
    "    intersect = math.pow(10, -1.5) * cutoff\n",
    "\n",
    "    if cutoff > 0 and (x - offset) < intersect or cutoff < 0 and (x - offset) > intersect:\n",
    "        return -(-cutoff/abs(cutoff) * x + cutoff/abs(cutoff) * offset + (math.pow(10, -1.5) * abs(cutoff)) + 1.5)\n",
    "\n",
    "    elif allow_further_opt:\n",
    "        return -min(1.5, -math.log10((x - offset) / cutoff))\n",
    "\n",
    "    else:\n",
    "        return -max(0, min(1.5, -math.log10((x - offset)/ cutoff)))\n",
    "\n",
    "# Load all models into memory\n",
    "for model_pref in preferences:\n",
    "\n",
    "    model_name = model_pref[\"model\"]\n",
    "    type = model_pref[\"type\"]\n",
    "\n",
    "    model_pref[\"clf\"] = joblib.load(model_directory + \"/\" + model_name + \".joblib\")\n",
    "\n",
    "annealing_temp = 1\n",
    "step_factor = 1\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    annealing_temp *= annealing_alpha\n",
    "    step_factor *= step_alpha\n",
    "\n",
    "    score = float()\n",
    "    model_scores = dict()\n",
    "\n",
    "    for model_pref in preferences:\n",
    "\n",
    "        model_name = model_pref[\"model\"]\n",
    "        type = model_pref[\"type\"]\n",
    "\n",
    "        if type == \"classification\":\n",
    "\n",
    "            result = model_pref[\"clf\"].predict_proba([latent])[0]\n",
    "            model_scores[model_name] = result.tolist()\n",
    "\n",
    "            for j in range(len(result)):\n",
    "\n",
    "                score += cost_function(result[j], model_pref[\"cutoff\"][j], offset=model_pref[\"offset\"][j], allow_further_opt=model_pref[\"further-optimisation\"][j]) * model_pref[\"weight\"][j]\n",
    "\n",
    "        elif type == \"regression\":\n",
    "\n",
    "            result = model_pref[\"clf\"].predict([latent])[0]\n",
    "            model_scores[model_name] = result.tolist()\n",
    "\n",
    "            score += cost_function(result, model_pref[\"cutoff\"], offset=model_pref[\"offset\"], allow_further_opt=model_pref[\"further-optimisation\"]) * model_pref[\"weight\"]\n",
    "\n",
    "    status = None\n",
    "\n",
    "    if score > previous_score:\n",
    "\n",
    "        previous_latent = latent\n",
    "        previous_score = score\n",
    "\n",
    "        status = \"Accepted\"\n",
    "\n",
    "        trajectory.append({\"iteration\": i + 1, \"score\": score, \"latent\": latent, \"model_scores\": model_scores})\n",
    "\n",
    "    else:\n",
    "\n",
    "        delta = score - previous_score\n",
    "        accept_probability = math.exp(beta * delta / annealing_temp)\n",
    "\n",
    "        if random.random() < accept_probability:\n",
    "\n",
    "            previous_latent = latent\n",
    "            previous_score = score\n",
    "\n",
    "            status = \"Accepted\"\n",
    "\n",
    "            trajectory.append({\"iteration\": i + 1, \"score\": score, \"latent\": latent, \"model_scores\": model_scores})\n",
    "\n",
    "        else:\n",
    "\n",
    "            latent = previous_latent\n",
    "\n",
    "            status = \"Declined\"\n",
    "\n",
    "    # Randomly move the points\n",
    "    latent = [x + random.gauss(0, step_std) for x in latent]\n",
    "\n",
    "    print(i, score, status, \"TEMPERATURE:\", annealing_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the results\n",
    "\n",
    "print(\"\\nFinal results\")\n",
    "print(\"===============\")\n",
    "\n",
    "for model_pref in preferences:\n",
    "\n",
    "    model_name = model_pref[\"model\"]\n",
    "    type = model_pref[\"type\"]\n",
    "\n",
    "    if type == \"classification\":\n",
    "\n",
    "        result = model_pref[\"clf\"].predict_proba([latent])[0]\n",
    "        print(model_name, result)\n",
    "\n",
    "    elif type == \"regression\":\n",
    "\n",
    "        result = model_pref[\"clf\"].predict([latent])[0]\n",
    "        print(model_name, result)\n",
    "\n",
    "print(\"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea07f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Typically potentiation works better on SVM models instead of ExtraTrees or other ensembling methods\n",
    "# This is because those create a nice continuous probability distribution\n",
    "# You can do a trade-off between the accuracy by changing the ExtraTreesClassifier in the earlier code segment to an SVM.\n",
    "# Or you can do some tuning with the hyperparameters to get the MCMC to work better, or just increase the number\n",
    "# of iterations.\n",
    "\n",
    "# Anyway, let us now look into the latent space of the final molecule that was accepted in the MCMC\n",
    "\n",
    "print(trajectory[-1])\n",
    "\n",
    "# You can decode this latent space by turning it back into fingerprints, or what I usually do is do a \n",
    "# search within a library of ZINC molecule latent spaces;\n",
    "# to see which one matches the best through Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us plot the trajectory of the entire potentiation, by doing a PCoA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as col\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "colours = [\"#bdc5c8\", \"#75818d\", \"#364354\", \"#0a0c10\"]\n",
    "cmap = col.LinearSegmentedColormap.from_list(\"BW\", colours, N=512)\n",
    "\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\": False, \"figure.autolayout\": True})\n",
    "\n",
    "all_iterations = list()\n",
    "all_latents = list()\n",
    "\n",
    "for trajectory_iteration in trajectory:\n",
    "\n",
    "    iteration = trajectory_iteration[\"iteration\"]\n",
    "    latent = trajectory_iteration[\"latent\"]\n",
    "\n",
    "    all_iterations.append(iteration)\n",
    "    all_latents.append(latent)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_latents)\n",
    "\n",
    "transformed = pca.transform(all_latents).T\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6.5, 6.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(\"black\")\n",
    "\n",
    "ax.tick_params(reset=True, color=\"black\", labelcolor=\"black\", direction=\"out\", which=\"both\", length=5, width=1, top=False, right=False, labelsize=11)\n",
    "\n",
    "points = ax.scatter(transformed[1], transformed[0], c=all_iterations, s=50, cmap=cmap)\n",
    "fig.colorbar(points)\n",
    "\n",
    "plt.ylabel(\"PC1 [{}%]\".format(round(pca.explained_variance_ratio_[0] * 100, 3)), fontsize=13)\n",
    "plt.xlabel(\"PC2 [{}%]\".format(round(pca.explained_variance_ratio_[1] * 100, 3)), fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82171b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also plot the trajectory of the BBB permeability\n",
    "\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\": True, \"figure.autolayout\": True})\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(\"black\")\n",
    "\n",
    "ax.tick_params(reset=True, color=\"black\", labelcolor=\"black\", direction=\"out\", which=\"both\", length=5, width=1, top=False, right=False, labelsize=13)\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for iteration in trajectory:\n",
    "\n",
    "    x.append(iteration[\"iteration\"])\n",
    "    y.append(iteration[\"model_scores\"][\"BBB_Martins\"][1])\n",
    "\n",
    "plt\n",
    "\n",
    "plt.plot(x, y, color=\"#364354\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8843a929",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "If you have any questions, please refer to the README for the contact details.\n",
    "\n",
    "Obligatory cat pic.\n",
    "\n",
    "![Logo](https://www.rd.com/wp-content/uploads/2021/01/GettyImages-1175550351.jpg \"Katze\")\n",
    "(Credit: Getty Images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
